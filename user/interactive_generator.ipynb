{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review response generation (demo)\n",
    "\n",
    "This is a demo script for automatic response generation models trained with Fairseq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "# set device\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"  # specify which GPU(s) to be used\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "from fairseq import bleu, checkpoint_utils, data, options, tasks, utils\n",
    "from fairseq.data import encoders\n",
    "from fairseq.logging.meters import StopwatchMeter\n",
    "\n",
    "\n",
    "# from fairseq.models.rrgen_seq2seq_lstm import rrgen_lstm_arch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define arguments and load model for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from /srv/scratch2/kew/fairseq_materials/rrgen/de/ft100src_rg/checkpoints/checkpoint_best.pt\n",
      "finished loading model from /srv/scratch2/kew/fairseq_materials/rrgen/de/ft100src_rg/checkpoints/checkpoint_best.pt\n"
     ]
    }
   ],
   "source": [
    "# Parse command-line arguments for generation\n",
    "parser = options.get_generation_parser(default_task='rrgen_translation')\n",
    "\n",
    "# test (small) model\n",
    "# input_args = [\n",
    "#     '/srv/scratch2/kew/fairseq_materials/translation_format/raw',\n",
    "#     '--path=/srv/scratch2/kew/fairseq_materials/translation_format/rrgen_lstm_emb_senti_cate_rate_100k/checkpoints/checkpoint_best.pt',\n",
    "#     '-s=src',\n",
    "#     '-t=tgt',\n",
    "#     '--task=rrgen_translation',\n",
    "#     '--dataset-impl=raw',\n",
    "#     '--nbest=1',\n",
    "#     '--beam=10',\n",
    "# #     '--sampling',\n",
    "# #     '--sampling-topp=0.9',\n",
    "#     '--use-sentiment=senti',\n",
    "#     '--use-category=cate',\n",
    "#     '--use-rating=rate',\n",
    "# ]\n",
    "\n",
    "input_args = [\n",
    "    '/srv/scratch2/kew/fairseq_materials/rrgen/de/data_bin_rg',\n",
    "    '--path=/srv/scratch2/kew/fairseq_materials/rrgen/de/ft100src_rg/checkpoints/checkpoint_best.pt',\n",
    "    '-s=review',\n",
    "    '-t=response_rg',\n",
    "    '--task=rrgen_translation',\n",
    "    '--dataset-impl=mmap',\n",
    "    '--nbest=1',\n",
    "    '--beam=5',\n",
    "#     '--sampling',\n",
    "#     '--sampling-topk=10',\n",
    "#     '--sampling-topp=0.98',\n",
    "    '--use-sentiment=sentiment',\n",
    "    '--use-category=domain',\n",
    "    '--use-rating=rating',\n",
    "]\n",
    "\n",
    "args = options.parse_args_and_arch(parser, input_args=input_args)\n",
    "\n",
    "# Set device\n",
    "# use_cuda = torch.cuda.is_available() and not args.cpu\n",
    "use_cuda = False\n",
    "\n",
    "# Setup task\n",
    "task = tasks.setup_task(args)\n",
    "\n",
    "# Load model\n",
    "print('loading model from {}'.format(args.path))\n",
    "models, _model_args = checkpoint_utils.load_model_ensemble(\n",
    "    [args.path], task=task)\n",
    "model = models[0]\n",
    "\n",
    "print('finished loading model from {}'.format(args.path))\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "# Load alignment dictionary for unknown word replacement\n",
    "# (None if no unknown word replacement, empty if no path to align dictionary)\n",
    "align_dict = utils.load_align_dict(args.replace_unk)\n",
    "\n",
    "# initialise generator model\n",
    "generator = task.build_generator(models, args)\n",
    "\n",
    "# Handle tokenization and BPE\n",
    "tokenizer = encoders.build_tokenizer(args)\n",
    "bpe = encoders.build_bpe(args)\n",
    "\n",
    "def decode_fn(x):\n",
    "    if bpe is not None:\n",
    "        x = bpe.decode(x)\n",
    "    if tokenizer is not None:\n",
    "        x = tokenizer.decode(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(sentence: str,\n",
    "                 ext_senti: Optional[int] = None,\n",
    "                 ext_cate: Optional[str] = None,\n",
    "                 ext_rate: Optional[int] = None,\n",
    "                 target: Optional[str] = None,\n",
    "                ):\n",
    "\n",
    "    # vectorize input sentence \n",
    "    tokens = task.source_dictionary.encode_line(\n",
    "    sentence, add_if_not_exist=False,)\n",
    "\n",
    "    if args.use_sentiment:\n",
    "        try:\n",
    "            ext_senti = task.ext_senti_dict[ext_senti]\n",
    "        except:\n",
    "            try:\n",
    "                ext_senti = task.ext_senti_dict[int(ext_senti)]\n",
    "            except:\n",
    "                r = random.choice(list(task.ext_senti_dict))\n",
    "                print(f'[!] WARNING: Could not find value for sentiment input {ext_senti}. Using sentiment value {r}')               \n",
    "                ext_senti = task.ext_senti_dict[r]\n",
    "    if args.use_category:\n",
    "        try:\n",
    "            ext_cate = task.ext_cate_dict[ext_cate]\n",
    "        except:\n",
    "            try:\n",
    "                ext_cate = task.ext_cate_dict[int(ext_cate)]\n",
    "            except:\n",
    "                r = random.choice(list(task.ext_cate_dict))\n",
    "                print(f'[!] WARNING: Could not find value for category input {ext_cate}. Using category value {r}')               \n",
    "                ext_cate = task.ext_cate_dict[r]\n",
    "    if args.use_rating:\n",
    "        try:\n",
    "            ext_rate = task.ext_rate_dict[ext_rate]\n",
    "        except:\n",
    "            try:\n",
    "                ext_rate = task.ext_rate_dict[int(ext_rate)]\n",
    "            except:\n",
    "                r = random.choice(list(task.ext_rate_dict))\n",
    "                print(f'[!] WARNING: Could not find value for category input {ext_rate}. Using rating value {r}')               \n",
    "                ext_rate = task.ext_rate_dict[r]\n",
    "                \n",
    "    # collate input as batch of size 1\n",
    "    batch = data.rrgen_dataset.collate(\n",
    "    samples=[{\n",
    "        'id': -1,\n",
    "        'source': tokens,\n",
    "        'ext_senti': ext_senti,\n",
    "        'ext_rate': ext_rate,\n",
    "        'ext_cate': ext_cate\n",
    "    }],\n",
    "    pad_idx=task.source_dictionary.pad(),\n",
    "    eos_idx=task.source_dictionary.eos(),\n",
    "    left_pad_source=False,\n",
    "    input_feeding=False)\n",
    "\n",
    "    # [!] ensure correct dtype (int64)\n",
    "    batch['net_input']['src_tokens'] = batch['net_input']['src_tokens'].type(\n",
    "        torch.LongTensor)\n",
    "\n",
    "    batch = utils.move_to_cuda(batch) if use_cuda else batch\n",
    "    \n",
    "    # assume no prefix tokens to initialise decoded output\n",
    "    prefix_tokens = None\n",
    "    \n",
    "    gen_timer = StopwatchMeter()\n",
    "    gen_timer.start()\n",
    "    # target_tokens = None\n",
    "    hypos = task.inference_step(generator, model, batch, prefix_tokens)\n",
    "    num_generated_tokens = sum(len(h[0]['tokens']) for h in hypos)\n",
    "    gen_timer.stop(num_generated_tokens)\n",
    "    \n",
    "    device = 'GPU' if use_cuda else 'CPU'\n",
    "    \n",
    "    print('Generated {} sentences ({} tokens) in {:.1f}s on {}\\n'.format(\n",
    "        batch['nsentences'], gen_timer.n, gen_timer.sum, device))\n",
    "    \n",
    "    # Process top predictions\n",
    "    for j, hypo in enumerate(hypos[0][:args.nbest]):\n",
    "        hypo_tokens, hypo_str, alignment = utils.post_process_prediction(\n",
    "            hypo_tokens=hypo['tokens'].int().cpu(),\n",
    "            src_str=tokens,\n",
    "            alignment=hypo['alignment'],\n",
    "            align_dict=align_dict,\n",
    "            tgt_dict=task.target_dictionary,\n",
    "            remove_bpe=args.remove_bpe,\n",
    "            # extra_symbols_to_ignore=get_symbols_to_strip_from_output(generator),\n",
    "        )\n",
    "        detok_hypo_str = decode_fn(hypo_str)\n",
    "        \n",
    "        print(detok_hypo_str)\n",
    "        print()\n",
    "        \n",
    "        if target:\n",
    "            # Generate and compute BLEU score\n",
    "            if args.sacrebleu:\n",
    "                scorer = bleu.SacrebleuScorer()\n",
    "            else:\n",
    "                scorer = bleu.Scorer(\n",
    "                    task.target_dictionary.pad(),\n",
    "                    task.target_dictionary.eos(),\n",
    "                    task.target_dictionary.unk())\n",
    "            \n",
    "#             if align_dict is not None or args.remove_bpe is not None:\n",
    "                # Convert back to tokens for evaluation with unk replacement and/or without BPE\n",
    "            target_tokens = task.target_dictionary.encode_line(target, add_if_not_exist=True)\n",
    "            hypo_tokens = task.target_dictionary.encode_line(detok_hypo_str, add_if_not_exist=True)\n",
    "            \n",
    "            if hasattr(scorer, 'add_string'): # i.e. if using SacreBLEU\n",
    "                scorer.add_string(target, detok_hypo_str)\n",
    "            else:\n",
    "                scorer.add(target_tokens, hypo_tokens) # calculate n-gram overlap on vectorized texts \n",
    "            \n",
    "            print(scorer.result_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate review responses with loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sentences (102 tokens) in 10.3s on CPU\n",
      "\n",
      "<GREETING> , vielen Dank , dass sie sich die Zeit genommen haben , uns ihre Meinung zu ihrem Aufenthalt bei uns mitzuteilen . auch wenn wir es bedauern , dass sie keine besseren Erfahrungen bei uns gemacht haben , hilft uns ihre Beurteilung dazu zu lernen und uns zu verbessern . wir arbeiten daran , es unseren Gästen an nichts fehlen zu lassen . wenn sie uns die Chance geben , ihr Vertrauen zurück zu gewinnen , garantieren wir ihnen , unser Bestes zu geben , damit sie genauso gute Erfahrungen mit uns machen können , wie andere Gäste . <SALUTATION>\n",
      "\n",
      "BLEU4 = 0.00, 43.6/12.0/1.0/0.0 (BP=0.837, ratio=0.849, syslen=101, reflen=119)\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"\n",
    "das Allerletzte ---SEP--- wir haben bereits vor Monaten \n",
    "zwei Doppelzimmer mit getrennten Betten gebucht , \n",
    "die Kreditkarte wurde lange vor der Ankunft belastet , \n",
    "obwohl die Zimmer bezahlt sind wird die Kreditkarte bei \n",
    "der Ankunft noch einmal mit <DIGIT> Euro Garantie belastet \n",
    "( mehr als der Preis der bereits bezahlten Übernachtungen ) , \n",
    "bei der Ankunft im Hotel ist die Reception völlig überlaufen , \n",
    "Wartezeit eine halbe Stunde , die reservierten und bezahlten \n",
    "Zimmer stehen nicht zur Verfügung , das Hotel sei ausgebucht , \n",
    "wir bekommen lediglich zwei Zimmer mit kleinen Doppelbetten , \n",
    "ein Zimmer nicht sehr sauber , es liegt im Eingangsbereich \n",
    "Müll auf dem Boden , das ganze ist eine Zumutung , \n",
    "Personal an der Rezeption genervt und unfreundlich , \n",
    "dieses Hotel besser meiden , \n",
    "P.S. : in der Hotelhalle ist auch noch Erbrochenes auf dem Fußboden\n",
    "\"\"\"\n",
    "target = \"\"\"\n",
    "<GREETING> W , vielen Dank für ihren Aufenthalt und für die Zeit ,\n",
    "die sie sich genommen haben um uns dieses Feedback zu schreiben .\n",
    "wir möchten uns natürlich für den negativen Eindruck ,\n",
    "den sie hier offensichtlich bekommen haben entschuldigen .\n",
    "da uns die Zufriedenheit unserer Gäste sehr am Herzen liegt ,\n",
    "möchten wir der Sache gerne auf den Grund gehen und bitten sie daher ,\n",
    "uns persönlich per E-Mail zu kontaktieren ,\n",
    "damit wir in diesem Fall recherchieren können .\n",
    "dies dient zum einen zur internen Verbesserung ,\n",
    "zum anderen möchten wir ihnen natürlich gerne zeigen ,\n",
    "dass wir es besser können und eine Lösung für diese Herausforderung finden .\n",
    "wir freuen uns auf ihre Rückmeldung .\n",
    "\"\"\"\n",
    "senti = 1\n",
    "cate = 1\n",
    "rate = 1\n",
    "\n",
    "get_response(sentence=review, ext_senti=senti, ext_cate=cate, ext_rate=rate, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sentences (49 tokens) in 3.7s on CPU\n",
      "\n",
      "<GREETING> , vielen Dank , dass sie sich die Zeit genommen haben , uns ihre Meinung zu ihrem Aufenthalt bei uns mitzuteilen . wir freuen uns , dass ihnen der Aufenthalt bei uns gefallen hat und hoffen , sie bald wieder bei uns begrüßen zu dürfen ! <SALUTATION>\n",
      "\n",
      "BLEU4 = 0.00, 29.2/8.5/2.2/0.0 (BP=1.000, ratio=1.116, syslen=48, reflen=43)\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"\n",
    "das war das letzte Mal ---SEP--- unprofessionelle , \n",
    "aggressive und gewalttätige Tür ohne jede Klasse und \n",
    "ohne die Fähigkeit gewaltfrei zu deeskalieren\"\"\"\n",
    "target = \"\"\"\n",
    "<GREETING> , wir bedauern sehr , dass sie so unangenehme\n",
    "eine Erfahrung bei uns machen Mussten . schreiben sie uns\n",
    "sehr gerne eine E-Mail an <EMAIL> , damit wir mehr Details\n",
    "über diesen Abend erfahren und gemeinsam eine Lösung finden können . <SALUTATION>\"\"\"\n",
    "senti = 1\n",
    "cate = 2\n",
    "rate = 1\n",
    "get_response(sentence=review, ext_senti=senti, ext_cate=cate, ext_rate=rate, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sentences (37 tokens) in 4.9s on CPU\n",
      "\n",
      "<GREETING> , vielen Dank , dass sie sich die Zeit genommen haben , uns ihre Meinung zu ihrem Aufenthalt bei uns mitzuteilen . wir hoffen , sie bald wieder bei uns begrüßen zu dürfen ! <SALUTATION>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"leider sehr enttäuschend ---SEP--- wir waren \n",
    "an einem sonnigen Donnerstagabend auf der Terrasse von The Butcher .\n",
    "auf unsere Speisekarte mussten wir erst einmal <DIGIT> Minuten warten ,\n",
    "die Bedienung erklärte uns , dass Zurzeit keine Karten verfügbar seien .\n",
    "die Bedienung war zwar freundliche , wirkte jedoch gestresst , \n",
    "weil alle Tische besetzt waren . \n",
    "wir bestellten zwei vegane Burger , da drei der vegetarischen Burger\n",
    "laut Karte Vegan zubereitet werden könne . die Bedienung erklärte uns dann aber ,\n",
    "dass dies nur bei einem Burger möglich sei . \n",
    "als die Burger nach einer halben Stunde serviert wurden , enthielten sie Käse .\n",
    "also schickten wir sie zurück in die Küche . \n",
    "nach einer Viertelstunde kamen die Burger zurück - diesmal mit einem \" V \" gekennzeichnet .\n",
    "sie enthielten noch immer Käse . erst nach insgesamt <DIGIT> Stunden Wartezeit\n",
    "erhielten wir schliesslich die bestellten veganen Burger , \n",
    "dabei handelte es sich aber eher um ein Brötchen mir Gemüse ohne Sauce . ich habe Verständnis dafür , dass nicht in jedem Restaurant Vegan gegessen werden kann . wer dies aber auf die Karte schreibt , sollte doch wenigstens wissen , was Vegan bedeutet . auch irritierend fand ich , dass ein Burger-Restaurant , das auf der Karte damit wirbt , besonders nachhaltig zu sein , alle Getränke im Plastik-Becher serviert . nachhaltig geht anders .\"\"\"\n",
    "senti = 2 # 7\n",
    "cate = 2\n",
    "rate = 1\n",
    "\n",
    "get_response(sentence=review, ext_senti=senti, ext_cate=cate, ext_rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sentences (102 tokens) in 12.3s on CPU\n",
      "\n",
      "<GREETING> , vielen Dank , dass sie sich die Zeit genommen haben , uns ihre Meinung zu ihrem Aufenthalt bei uns mitzuteilen . auch wenn wir es bedauern , dass sie keine besseren Erfahrungen bei uns gemacht haben , hilft uns ihre Beurteilung dazu zu lernen und uns zu verbessern . wir arbeiten daran , es unseren Gästen an nichts fehlen zu lassen . wenn sie uns die Chance geben , ihr Vertrauen zurück zu gewinnen , garantieren wir ihnen , unser Bestes zu geben , damit sie genauso gute Erfahrungen mit uns machen können , wie andere Gäste . <SALUTATION>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"falsche Zusagen ( schriftlich ) ---SEP--- unsere Anreise erfolgte gegen <DIGIT> Uhr mit <DIGIT> müden Kindern nach einer <DIGIT> stündigen Autofahrt . ich wurde noch begrüßt , mein Mann der <DIGIT> Min später nach kam nicht ( kein Problem ) . vor der Buchung wurde uns schriftlich ein Doppelzimmer mit Verbindungstür bestätigt . vor Ort , wusste keiner etwas davon . Kinderzimmer auf einer Etage , Elternschlafzimmer auf einer anderen ! ? ? ? nach eindringlichem Bitten eine Lösung zu finden , wurde mir gesagt sie seien ausgebucht und das ist eben so ! ? ich habe wehement darum gebeten eine Lösung zu finden - beide angestellte <DIGIT> Min weg ! ? dann wurde mir gesagt , sie versuchen es- kann dauern ..... auf meine Frage , wie lange ( mittlerweile <DIGIT> Uhr ) war die Antwort - keine Ahnung 🤔 . unfassbar wie Unprofessionell . nach eindringlichem Bitten mir eine ungefähre Zeit zu nennen , <DIGIT> Min , <DIGIT> Min oder eine Stunde wurde mir gesagt eher 1stunde . ich habe dankend abgelehnt und darum gebeten den Vorgang zu stornieren . ich hätte dieses Hotel niemals ohne Zusage der Verbindungstür gewählt . vor Ort waren die Mitarbeiter echt frech und ignorant - ich bin schwer enttäuscht . das einzig positive , das sie den Vorgang ohne Kosten storniert haben\"\"\"\n",
    "senti = 1\n",
    "cate = 1\n",
    "rate = 1\n",
    "\n",
    "get_response(sentence=review, ext_senti=senti, ext_cate=cate, ext_rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sentences (49 tokens) in 4.7s on CPU\n",
      "\n",
      "<GREETING> , vielen Dank , dass sie sich die Zeit genommen haben , uns ihre Meinung zu ihrem Aufenthalt bei uns mitzuteilen . wir freuen uns , dass ihnen der Aufenthalt bei uns gefallen hat und hoffen , sie bald wieder bei uns begrüßen zu dürfen ! <SALUTATION>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"Miserabel ---SEP--- im Hotel Seiler ist das Wort Kulanz nicht existent . wir hätten wenigstens ein Entgegenkommen in Form.Z . b.eines Gutscheines erwartet . das Hotel Seiler haben wir aus unserer Liste gestrichen . wir werden auch entsprechend Werbung für das Haus machen .\"\"\"\n",
    "senti = 4\n",
    "cate = 1\n",
    "rate = 1\n",
    "get_response(sentence=review, ext_senti=senti, ext_cate=cate, ext_rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sentences (49 tokens) in 4.7s on CPU\n",
      "\n",
      "<GREETING> , vielen Dank , dass sie sich die Zeit genommen haben , uns ihre Meinung zu ihrem Aufenthalt bei uns mitzuteilen . wir freuen uns , dass ihnen der Aufenthalt bei uns gefallen hat und hoffen , sie bald wieder bei uns begrüßen zu dürfen ! <SALUTATION>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"stinkend statt dufte ---SEP--- das gesamte Hotel eoch nach Toilette , da es ein paar Wochen zuvor ein Abwasser-Rohrbruch gab . das Badwar ungenügend geputzt und die Klimaanlage ließ sich nicht regeln . das Zimmer war sehr warm . das Personal am Check-In lehnte eine Reklamation ab und wies mehrere Gäste in arroganter und unfreundlicher Weise ab .\"\"\"\n",
    "senti = 3 #7\n",
    "cate = 1\n",
    "rate = 1\n",
    "get_response(sentence=review, ext_senti=senti, ext_cate=cate, ext_rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sentences (101 tokens) in 9.4s on CPU\n",
      "\n",
      "<GREETING> vielen Dank , dass sie sich die Zeit genommen haben , uns ihre Meinung zu ihrem Aufenthalt bei uns mitzuteilen . auch wenn wir es bedauern , dass sie keine besseren Erfahrungen bei uns gemacht haben , hilft uns ihre Beurteilung dazu zu lernen und uns zu verbessern . wir arbeiten daran , es unseren Gästen an nichts fehlen zu lassen . wenn sie uns die Chance geben , ihr Vertrauen zurück zu gewinnen , garantieren wir ihnen , unser Bestes zu geben , damit sie genauso gute Erfahrungen mit uns machen können , wie andere Gäste . <SALUTATION>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"enttäuschend ---SEP--- der Wein ( Pinot Grigio und Frascati ) war absolut nicht zu empfehlen , billige No-Name Ware , das Essen geschmacklos ( Pizza Frutti Di Mare mit Meeresfrüchte-Imitat aus Surimi ) . die Damen im Service freundlich jedoch der Patrone beim Bezahlen der Rechnung unfreundlich und arrogant . nie wieder !\"\"\"\n",
    "senti = 3\n",
    "cate = 2\n",
    "rate = 1\n",
    "get_response(sentence=review, ext_senti=senti, ext_cate=cate, ext_rate=rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sentences (49 tokens) in 4.7s on CPU\n",
      "\n",
      "<GREETING> , vielen Dank , dass sie sich die Zeit genommen haben , uns ihre Meinung zu ihrem Aufenthalt bei uns mitzuteilen . wir freuen uns , dass ihnen der Aufenthalt bei uns gefallen hat und hoffen , sie bald wieder bei uns begrüßen zu dürfen ! <SALUTATION>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"es war besser als befriedigend aber nicht sehr gut ---SEP--- Gesamteindruck super , Preise für Garage und Frühstück sind generell in allen Hotels zu hoch . Preis / Nacht wäre besser zu akzeptieren wenn es Fitness/Wellness O.Ä. geben würde . dazu könnten Z.B. Info ´ s zu Partnern ausliegen .\"\"\"\n",
    "senti = 6\n",
    "cate = 1\n",
    "rate = 3\n",
    "get_response(sentence=review, ext_senti=senti, ext_cate=cate, ext_rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sentences (37 tokens) in 4.7s on CPU\n",
      "\n",
      "<GREETING> , vielen Dank , dass sie sich die Zeit genommen haben , uns ihre Meinung zu ihrem Aufenthalt bei uns mitzuteilen . wir hoffen , sie bald wieder bei uns begrüßen zu dürfen ! <SALUTATION>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"Weihnachten im Parkhotel lindner Oberstaufen ---SEP--- unvergessliche Weihnachtsatmosphäre im Parkhotel Lindner , zusammen feiern in einer grossen Familie , sehr gediegen mit Harfenmusik , Weihnachtsgeschichten hören und zusammen Weihnachtslieder singen unter einem wunderbar geschmückten Baum , das war Weihnachten <DIGIT> . das freundliche aufgestellte Personal verwöhnte uns Gäste mit auserlesenen Speisen aus der Küche von Herrn Wagenblast . es war einmal mehr ein unvergesslicher Aufenthalt .\"\"\"\n",
    "senti = 8\n",
    "cate = 1\n",
    "rate = 5\n",
    "get_response(sentence=review, ext_senti=senti, ext_cate=cate, ext_rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
